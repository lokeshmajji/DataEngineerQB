# Hive Questions

1) What is partitioning
        Partitioning – Apache Hive organizes tables into partitions for grouping same type of data together based on a column or partition key. 
        Each table in the hive can have one or more partition keys to identify a particular partition.
            Using partition we can make it faster to do queries on slices of the data.

        CREATE TABLE table_name (column1 data_type, column2 data_type) PARTITIONED BY (partition1 data_type, partition2 data_type,….);
        
        create table p_patient1(patient_id int, patient_name string, gender string, total_amount int) partitioned by ( drug string);
        insert overwrite table p_patient1 partition(drug='metacin') select patient_id, patient_name, gender, total_amount from patient where drug='metacin';

        SET hive.exec.dynamic.partition=true;
        SET hive.exec.dynamic.partition.mode=non-strict;
        create table dynamic_partition_patient (patient_id int,patient_name string, gender string, total_amount int) partitioned by (drug string);
        insert into table dynamic_partition_patient PARTITION(drug) select * from patient1;

2) What is Bucketing
        In Hive Tables or partition are subdivided into buckets based on the hash function of a column in the table to give extra structure to the data that may be used for more efficient queries.

        CREATE TABLE table_name PARTITIONED BY (partition1 data_type, partition2 data_type,….) CLUSTERED BY (column_name1, column_name2, …) SORTED BY (column_name [ASC|DESC], …)] INTO num_buckets BUCKETS;

        create table patient(patient_id int, patient_name string, drug string, gender string, total_amount int) row format delimited fields terminated by ',' stored as textfile;
        load data local inpath '/home/geouser/Documents/patient' into table patient;
        
        set hive.enforce.bucketing =true;
        create table bucket_patient(patient_id int, patient_name string, drug string,gender string, total_amount int) clustered by (drug) into 4 buckets;
        insert overwrite table bucket_patient select * from patient;
        select * from bucket_patient TABLESAMPLE(BUCKET 1 OUT OF 4 ON drug);
        select * from bucket_patient TABLESAMPLE(10 percent);


3) What is the difference between external table and managed table?
In case of managed table, If one drops a managed table, the metadata information along with the table data is deleted from the Hive warehouse directory.
On the contrary, in case of an external table, Hive just deletes the metadata information regarding the table and leaves the table data present in HDFS untouched. 

4) How will you consume this CSV file into the Hive warehouse using built SerDe?
        id first_name last_name email gender ip_address
        1 Hugh Jackman hughjackman@cam.ac.uk Male 136.90.241.52
        2 David Lawrence dlawrence1@gmail.com Male 101.177.15.130        
        
        CREATE EXTERNAL TABLE sample(id int, first_name string,last_name string, email string,gender string, ip_address string) 
            ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.OpenCSVSerde’ 
            STORED AS TEXTFILE LOCATION ‘/temp’;

        SELECT first_name FROM sample WHERE gender = ‘male’;

5) Suppose, I have a lot of small CSV files present in /input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. 
Now, as we know, Hadoop performance degrades when we use lots of small files.So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

        CREATE TABLE temp_table (id INT, name STRING, e-mail STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;
        LOAD DATA INPATH ‘/input’ INTO TABLE temp_table;
        
        CREATE TABLE sample_seqfile (id INT, name STRING, e-mail STRING, country STRING) ROW FORMAT DELIMITED STORED AS SEQUENCEFILE;
        INSERT OVERWRITE TABLE sample SELECT * FROM temp_table;


6) Transer data to HDFS
        hdfs dfs -put /home/cloudera/Desktop/Loki/employees/employees.json /user/cloudera/employees/
        hdfs dfs -put /home/cloudera/Desktop/Loki/employees/salaries.json /user/cloudera/employees/
        
        val sal = spark.read.json("/user/cloudera/employees/salaries.json")
        create table emp_sal(emp_no string,from_date string,salary int,to_date string) stored as parquetfile LOCATION '/user/cloudera/out/emp/parquet';



